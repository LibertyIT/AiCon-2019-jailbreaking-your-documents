{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Scikit-learn\n",
    "\n",
    "## Overview\n",
    "In this example we will use scikit-learn to train a Naive Bayes classifier using the 20 news groupd data set that is built into scikit-learn.\n",
    "\n",
    "## Resources\n",
    "Below is the resources that we used to put together this example:\n",
    "- https://scikit-learn.org/\n",
    "- https://scikit-learn.org/0.16/datasets/index.html\n",
    "- https://seaborn.pydata.org/\n",
    "- Book : Python Machine Learning By Example - Second Edition by Yuxi Liu\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Training Data\n",
    "\n",
    "\"20 Newsgroup\" - Scikit built in data set\n",
    "\n",
    "- https://scikit-learn.org/0.16/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups \n",
    "- https://github.com/scikit-learn/scikit-learn/blob/a95203b/sklearn/datasets/twenty_newsgroups.py#L151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Use 4 of the 20 categories\n",
    "sub_catergory_list = ['rec.autos', 'rec.motorcycles',\n",
    "                      'rec.sport.baseball','rec.sport.hockey']\n",
    "\n",
    "# Get training data\n",
    "training_data = fetch_20newsgroups(subset='train', \n",
    "                                   categories=sub_catergory_list, \n",
    "                                   shuffle=True, download_if_missing=False)\n",
    "\n",
    "print('Training Categories:', list(training_data.target_names))\n",
    "print('Total Training Data:', len(training_data.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Analysis\n",
    "It’s good to get an understanding of your dataset. In relation to multiple classifications categories, it is important to know the distribution of the categories so categories are then not under or overrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.distplot(training_data.target)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Converting Text to Vectors\n",
    "To feed predictive models with text data, we first most turn text into vectors of numerical values suitable for statistical analysis.\n",
    "\n",
    "We will use scikit-learn 'CountVectorizer' to create feature vectors -\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(test):\n",
    "    training_data_cleaned = []\n",
    "    for data in training_data.data:\n",
    "        data_cleaned = ' '.join(word for word in data.split() \n",
    "                            if letter_only(word))\n",
    "        training_data_cleaned.append(data_cleaned)\n",
    "    \n",
    "    return training_data_cleaned\n",
    "    \n",
    "def letter_only(word):\n",
    "    for character in word:\n",
    "        if not character.isalpha():\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "# To ensure we don't have numbers included in our feature words we will remove them.\n",
    "training_data_cleaned = clean_data(training_data)\n",
    "\n",
    "# Create count vectorizers with cleaned data and remove stop words. \n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", max_features=5000)\n",
    "x_training_count =  count_vectorizer.fit_transform(training_data_cleaned)\n",
    "\n",
    "print('Document-Term Matrix:' + '[n_samples, n_features]: ', x_training_count.shape)\n",
    "\n",
    "list = count_vectorizer.get_feature_names()\n",
    "print(list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert Count Matrix to normalised tf-idf\n",
    "\n",
    "__Tf__ : term-frequency, prevent giving longer text documents more weightage than shorter documents.\n",
    "\n",
    "__tf-idf__ : term-frequency - inverse document-frequency, intended to reflect how important a word <br> is to a document in a corpus.  \n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "x_training_tfidf = tfidf_transformer.fit_transform(x_training_count)\n",
    "\n",
    "print('Document-Term Matrix: [n_samples, n_features_new]', x_training_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build NBclassifier (Naive Bayes) - Train Classifier\n",
    "\n",
    "We will use the MultinomialNB which implements the NB algorithm for multinomially distributed data.\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "- https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n",
    "\n",
    "We use the MultinomialNB which implements the NB alogorithm for multinomially distributed data.\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Fit the classifier according to x (training vectors) and y (target values)\n",
    "classifier = MultinomialNB().fit(x_training_tfidf, training_data.target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - 5 can be replaced by using a sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_classifier = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\n",
    "classifier = text_clf.fit(training_data.data, training_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Built Classifier\n",
    "Get test data from the 20 News Group data set (same categories as training data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "testing_data = fetch_20newsgroups(subset='test', categories=sub_catergory_list, shuffle=True, download_if_missing=False)\n",
    "\n",
    "print('Total Testing Data:', len(testing_data.data))\n",
    "\n",
    "# Transform Test data vectors \n",
    "x_testing_count = count_vectorizer.transform(testing_data.data)\n",
    "x_testing_tfidf = tfidf_transformer.fit_transform(x_testing_count)\n",
    "\n",
    "# Perform prediction on test data \n",
    "predictions = classifier.predict(x_testing_tfidf)\n",
    "np.mean(predictions == testing_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction on test data \n",
    "predictions = classifier.predict(x_testing_tfidf)\n",
    "\n",
    "np.mean(predictions == testing_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(testing_data.target, predictions)\n",
    "metrics_report = classification_report(testing_data.target,  predictions)\n",
    "cm = confusion_matrix(testing_data.target, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy rate: \" + str(round(accuracy, 2)) + \"\\n\")\n",
    "print(\"Confusion Matrix Summary \\n\\n\", cm)\n",
    "print(\"Report: \\n\\n\", metrics_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [18,18]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix graph')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + training_data.target_names)\n",
    "ax.set_yticklabels([''] + training_data.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify John's Invoice Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_auto = \"Small Auto Parts Supplier Rep Supplier Katie Russ, Payment Terms Due Date Due on receipt 02-12-2019 Qty Description 1 Wheel 2 Wax 1 Battery 2 Headlight bulb 1 Clutch INVOICE Date: 01-01-2019 INVOCE#100 John Smith, Smith's Automobiles 100 Long Roadway, Belfast BT32 QQU 028 90 222222 Customer ID ABC12345 Address 23 Seymour Drive Lisburn Northern Ireland Unit Price £40.00 £10.00 £55.00 £8.00 £140.00 Subtotal Sales Tax Total Make all checks payable to Sample Test Invoice Thank you for your business! TOTAL £40.00 £20.00 £55.00 £16.00 £140.00 £271.00 £120.00 £391.00\"\n",
    "extracted_moto = \"Big Bike Parts Supplier Rep Supplier Seamus White Big Bike Parts Payment Terms Due Date Due on receipt 12.23.2019 Qty Description 1 Wheel 2 Wax 1 Battery 2 Headlight bulb 1 Motorcycle Fork INVOICE Date: 01/01/2019 INVOICE # 100 John Smith Smith’s Automobiles 100 Long Roadway, Belfast, Co. Antrim, BT32 QQU 028 90 222222 Customer ID ABC12345 Address 55 Prospect Drive, Belfast, Co. Antrim Unit Price £40.00 £10.00 £55.00 £8.00 £140.00 Subtotal Sales Tax Total Make all checks payable to Sample Test Invoice Thank you for your business! Line Total £40.00 £20.00 £55.00 £16.00 £140.00 £271.00 £120.00 £391.00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data_list = [extracted_auto, extracted_moto]\n",
    "\n",
    "test_count = count_vectorizer.transform(extracted_data_list)\n",
    "test_tfidf = tfidf_transformer.fit_transform(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicition = classifier.predict(test_tfidf)\n",
    "\n",
    "for t in test_predicition:\n",
    "    print(training_data.target_names[t])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
